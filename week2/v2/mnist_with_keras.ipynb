{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST digits classification with Keras\n",
    "\n",
    "We don't expect you to code anything here because you've already solved it with TensorFlow.\n",
    "\n",
    "But you can appreciate how simpler it is with Keras.\n",
    "\n",
    "We'll be happy if you play around with the architecture though, there're some tips at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnist_sample.png\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 1.10.0\n",
      "We are using Keras 2.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(\"We're using TF\", tf.__version__)\n",
    "import keras\n",
    "print(\"We are using Keras\", keras.__version__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import keras_utils\n",
    "from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data\n",
    "\n",
    "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessed_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [shape (50000, 28, 28)] sample patch:\n",
      " [[0.         0.29803922 0.96470588 0.98823529 0.43921569]\n",
      " [0.         0.33333333 0.98823529 0.90196078 0.09803922]\n",
      " [0.         0.33333333 0.98823529 0.8745098  0.        ]\n",
      " [0.         0.33333333 0.98823529 0.56862745 0.        ]\n",
      " [0.         0.3372549  0.99215686 0.88235294 0.        ]]\n",
      "A closeup of a sample patch:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the whole sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [shape (50000,)] 10 samples:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# X contains rgb values divided by 255\n",
    "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print(\"A closeup of a sample patch:\")\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"And the whole sample:\")\n",
    "plt.imshow(X_train[1], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# flatten images\n",
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_conv = np.expand_dims(X_train,axis=3)\n",
    "print(X_train_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the target\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n"
     ]
    }
   ],
   "source": [
    "# building a model with keras\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Sequential\n",
    "\n",
    "# we still need to clear a graph though\n",
    "s = reset_tf_session()\n",
    "\n",
    "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
    "model.add(Dense(300, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# now we \"compile\" the model specifying the loss and optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # this is our cross-entropy\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "from keras import backend as K\n",
    "fun = K.function([model.layers[0].input],[model.layers[0].output])\n",
    "\n",
    "x_inp = X_train_flat\n",
    "layer_output = fun([x_inp])[0]\n",
    "\n",
    "print(layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 26, 26, 16)\n"
     ]
    }
   ],
   "source": [
    "# building a model with keras\n",
    "from keras.layers import Dense, Activation, Input, Conv2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# we still need to clear a graph though\n",
    "s = reset_tf_session()\n",
    "\n",
    "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
    "model.add(Conv2D(16, (3,3), activation = 'relu', input_shape=(28,28,1)))\n",
    "# model.add(Dense(300, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('tanh'))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# now we \"compile\" the model specifying the loss and optimizer\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy', # this is our cross-entropy\n",
    "#     optimizer='adam',\n",
    "#     metrics=['accuracy']  # report accuracy during training\n",
    "# )\n",
    "\n",
    "from keras import backend as K\n",
    "fun = K.function([model.layers[0].input],[model.layers[0].output])\n",
    "\n",
    "x_inp = X_train_conv\n",
    "layer_output = fun([x_inp])[0]\n",
    "\n",
    "print(layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "??keras.layers.Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# you can look at all layers and parameter count\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a method into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1040\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1041\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3338\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3427\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3428\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a method into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-3cbc7674668e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1041\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1043\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a method into a Tensor."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we \"compile\" the model specifying the loss and optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # this is our cross-entropy\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "??model.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 1s 21us/step - loss: 0.4446 - acc: 0.8725 - val_loss: 0.2425 - val_acc: 0.9320\n",
      "Epoch 2/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.2318 - acc: 0.9328 - val_loss: 0.1908 - val_acc: 0.9460\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.1752 - acc: 0.9492 - val_loss: 0.1505 - val_acc: 0.9582\n",
      "Epoch 4/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.1359 - acc: 0.9607 - val_loss: 0.1289 - val_acc: 0.9636\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.1103 - acc: 0.9678 - val_loss: 0.1140 - val_acc: 0.9674\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0902 - acc: 0.9736 - val_loss: 0.1014 - val_acc: 0.9693\n",
      "Epoch 7/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0764 - acc: 0.9773 - val_loss: 0.0983 - val_acc: 0.9711\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0632 - acc: 0.9818 - val_loss: 0.0873 - val_acc: 0.9741\n",
      "Epoch 9/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0526 - acc: 0.9850 - val_loss: 0.0888 - val_acc: 0.9712\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0446 - acc: 0.9876 - val_loss: 0.0807 - val_acc: 0.9746\n",
      "Epoch 11/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0377 - acc: 0.9894 - val_loss: 0.0775 - val_acc: 0.9753\n",
      "Epoch 12/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0305 - acc: 0.9921 - val_loss: 0.0751 - val_acc: 0.9771\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0264 - acc: 0.9935 - val_loss: 0.0752 - val_acc: 0.9761\n",
      "Epoch 14/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0205 - acc: 0.9955 - val_loss: 0.0718 - val_acc: 0.9771\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0169 - acc: 0.9967 - val_loss: 0.0793 - val_acc: 0.9749\n",
      "Epoch 16/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0148 - acc: 0.9972 - val_loss: 0.0748 - val_acc: 0.9779\n",
      "Epoch 17/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0115 - acc: 0.9983 - val_loss: 0.0776 - val_acc: 0.9767\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0100 - acc: 0.9985 - val_loss: 0.0734 - val_acc: 0.9783\n",
      "Epoch 19/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0083 - acc: 0.9988 - val_loss: 0.0748 - val_acc: 0.9775\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0068 - acc: 0.9993 - val_loss: 0.0750 - val_acc: 0.9780\n",
      "Epoch 21/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0054 - acc: 0.9996 - val_loss: 0.0748 - val_acc: 0.9792\n",
      "Epoch 22/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0049 - acc: 0.9996 - val_loss: 0.0776 - val_acc: 0.9793\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.0756 - val_acc: 0.9797\n",
      "Epoch 24/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0750 - val_acc: 0.9802\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 0.9798\n",
      "Epoch 26/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0024 - acc: 0.9998 - val_loss: 0.0762 - val_acc: 0.9798\n",
      "Epoch 27/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0771 - val_acc: 0.9792\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 0.9794\n",
      "Epoch 29/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0789 - val_acc: 0.9801\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0787 - val_acc: 0.9797\n",
      "Epoch 31/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0807 - val_acc: 0.9794\n",
      "Epoch 32/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0800 - val_acc: 0.9795\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 9.0961e-04 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9796\n",
      "Epoch 34/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 8.1828e-04 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9799\n",
      "Epoch 35/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 7.0571e-04 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9808\n",
      "Epoch 36/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 6.4024e-04 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 0.9805\n",
      "Epoch 37/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 5.8051e-04 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9806\n",
      "Epoch 38/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 5.3493e-04 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9803\n",
      "Epoch 39/40\n",
      "50000/50000 [==============================] - 1s 19us/step - loss: 4.9415e-04 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9806\n",
      "Epoch 40/40\n",
      "50000/50000 [==============================] - 1s 18us/step - loss: 4.4563e-04 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a31c7a9e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we can fit the model with model.fit()\n",
    "# and we don't have to write loops and batching manually as in TensorFlow\n",
    "model.fit(\n",
    "    X_train_flat, \n",
    "    y_train_oh,\n",
    "    batch_size=512, \n",
    "    epochs=40,\n",
    "    validation_data=(X_val_flat, y_val_oh),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here're the notes for those who want to play around here\n",
    "\n",
    "Here are some tips on what you could do:\n",
    "\n",
    " * __Network size__\n",
    "   * More neurons, \n",
    "   * More layers, ([docs](https://keras.io/))\n",
    "\n",
    "   * Other nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "\n",
    " * __Early Stopping__\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "     \n",
    "\n",
    " * __Faster optimization__\n",
    "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "\n",
    "\n",
    " * __Regularize__ to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "     * Can be done manually or via - https://keras.io/regularizers/\n",
    "   \n",
    "   \n",
    " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
    "   * https://keras.io/preprocessing/image/\n",
    "   * Zoom-in+slice = move\n",
    "   * Rotate+zoom(to remove black stripes)\n",
    "   * any other perturbations\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 rl",
   "language": "python",
   "name": "python3_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
